{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "# Routers\n",
    "\n",
    "# Rankers\n",
    "# Retrievers\n",
    "# GPTGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = os.getenv(\"OAI_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ferdydh/Code/haystack-rag-showcase/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 100/100 [00:43<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin, urlunparse\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import urllib3\n",
    "from haystack.dataclasses import ByteStream\n",
    "\n",
    "# Set the logging level for urllib3 to WARNING to suppress debug messages\n",
    "urllib3_logger = logging.getLogger('urllib3')\n",
    "urllib3_logger.setLevel(logging.WARNING)\n",
    "\n",
    "def can_fetch(url, user_agent='*'):\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(urljoin(url, '/robots.txt'))\n",
    "    rp.read()\n",
    "\n",
    "    return rp.can_fetch(user_agent, url)\n",
    "\n",
    "def is_media_url(url):\n",
    "    media_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif']\n",
    "    return any(url.lower().endswith(ext) for ext in media_extensions)\n",
    "\n",
    "result = []\n",
    "\n",
    "def crawl_website(root_url, max_workers=15, fetch_count=100):\n",
    "    search_domain = urlparse(root_url).hostname    \n",
    "    visited_urls = set()\n",
    "    stack = [root_url]\n",
    "    visited_count = 0\n",
    "\n",
    "    def process_page(url):\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            visited_urls.add(url)\n",
    "\n",
    "            # Process the page here, e.g., extract information or save data\n",
    "            result.append({\"url\":url, \"content\": ByteStream.from_string(response.text)})\n",
    "\n",
    "            # Find and collect outgoing links\n",
    "            outgoing_links = [urlparse(urljoin(url, link['href'])) for link in soup.find_all('a', href=True)]\n",
    "            outgoing_stripped_urls = [urlunparse((next_url.scheme, next_url.netloc, next_url.path, '', '', '')) for next_url in outgoing_links]\n",
    "\n",
    "            # Filter out media URLs\n",
    "            non_media_links = [outgoing_url for outgoing_url in outgoing_stripped_urls if not is_media_url(outgoing_url)]\n",
    "\n",
    "            return [non_media_url for non_media_url in non_media_links if search_domain in non_media_url]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling {url}: {e}\")\n",
    "            return []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers) as executor, tqdm(total=fetch_count) as pbar:\n",
    "        while stack:\n",
    "            if (visited_count >= fetch_count):\n",
    "                return\n",
    "            \n",
    "            current_url = stack.pop()\n",
    "\n",
    "            if (current_url in visited_urls) or (not search_domain in current_url) or (not can_fetch(current_url)):\n",
    "                continue\n",
    "\n",
    "            # Submit the processing of the page to the thread pool\n",
    "            future = executor.submit(process_page, current_url)\n",
    "\n",
    "            # Collect outgoing links from the processed page\n",
    "            outgoing_links = future.result()\n",
    "\n",
    "            # Add outgoing links to the stack\n",
    "            stack.extend(filter(lambda url: url not in visited_urls, outgoing_links))\n",
    "            \n",
    "            visited_count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "# Example usage:\n",
    "crawl_website('https://www.cit.tum.de', fetch_count=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: SAX input contains nested A elements -- You have probably hit a bug in your HTML parser (e.g., NekoHTML bug #2909310). Please clean the HTML externally and feed it to BoilerPy3 again. Trying to recover somehow...\n",
      "Batches: 100%|██████████| 2/2 [00:07<00:00,  3.93s/it]\n",
      "Batches: 100%|██████████| 4/4 [00:08<00:00,  2.15s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metadata_router': {'unmatched': []},\n",
       " 'en_writer': {'documents_written': 64},\n",
       " 'de_writer': {'documents_written': 107}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifier\n",
    "# Routers\n",
    "\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.classifiers import DocumentLanguageClassifier\n",
    "from haystack.components.routers import MetadataRouter\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "# Document Store\n",
    "document_store_en = InMemoryDocumentStore()\n",
    "document_store_de = InMemoryDocumentStore()\n",
    "\n",
    "\n",
    "en_writer = DocumentWriter(document_store = document_store_en)\n",
    "de_writer = DocumentWriter(document_store = document_store_de)\n",
    "\n",
    "\n",
    "# Data pipeline\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"converter\", HTMLToDocument())\n",
    "pipeline.add_component(\"cleaner\", DocumentCleaner())\n",
    "pipeline.add_component(\"splitter\", DocumentSplitter(split_by=\"sentence\", split_length=30))\n",
    "\n",
    "\n",
    "# Language Pipeline Components\n",
    "document_classifier = DocumentLanguageClassifier(languages = [\"en\", \"de\", \"id\"])\n",
    "metadata_router = MetadataRouter(rules={\"en\": {\"language\": {\"$eq\": \"en\"}}, \"de\": {\"language\": {\"$eq\": \"de\"}}})\n",
    "english_embedder = SentenceTransformersDocumentEmbedder()\n",
    "german_embedder = SentenceTransformersDocumentEmbedder(model_name_or_path=\"PM-AI/bi-encoder_msmarco_bert-base_german\")\n",
    "\n",
    "pipeline.add_component(instance=document_classifier, name=\"document_classifier\")\n",
    "pipeline.add_component(instance=metadata_router, name=\"metadata_router\")\n",
    "pipeline.add_component(instance=english_embedder, name=\"english_embedder\")\n",
    "pipeline.add_component(instance=german_embedder, name=\"german_embedder\")\n",
    "pipeline.add_component(instance=en_writer, name=\"en_writer\")\n",
    "pipeline.add_component(instance=de_writer, name=\"de_writer\")\n",
    "\n",
    "# Connect all\n",
    "pipeline.connect(\"converter\", \"cleaner\")\n",
    "pipeline.connect(\"cleaner\", \"splitter\")\n",
    "pipeline.connect(\"splitter\", \"document_classifier.documents\")\n",
    "\n",
    "pipeline.connect(\"document_classifier.documents\", \"metadata_router.documents\")\n",
    "pipeline.connect(\"metadata_router.en\", \"english_embedder.documents\")\n",
    "pipeline.connect(\"metadata_router.de\", \"german_embedder.documents\")\n",
    "pipeline.connect(\"english_embedder\", \"en_writer\")\n",
    "pipeline.connect(\"german_embedder\", \"de_writer\")\n",
    "\n",
    "\n",
    "pipeline.run(\n",
    "    {\n",
    "        \"converter\": {\n",
    "            \"sources\": [x[\"content\"] for x in result],\n",
    "            \"meta\": [{\"url\":x[\"url\"]} for x in result]\n",
    "            }\n",
    "        }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rankers\n",
    "# Retrievers\n",
    "# GPTGenerator\n",
    "\n",
    "from haystack import Pipeline\n",
    "from langdetect import detect\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.rankers import TransformersSimilarityRanker\n",
    "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "# English \n",
    "english_pipeline = Pipeline()\n",
    "english_pipeline.add_component(\"text_embedder\", SentenceTransformersTextEmbedder())\n",
    "english_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store_en, top_k=50))\n",
    "english_pipeline.add_component(\"ranker\", TransformersSimilarityRanker())\n",
    "english_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "english_pipeline.connect(\"retriever.documents\", \"ranker.documents\")\n",
    "\n",
    "# German\n",
    "german_pipeline = Pipeline()\n",
    "german_pipeline.add_component(\"text_embedder\", SentenceTransformersTextEmbedder(model_name_or_path=\"PM-AI/bi-encoder_msmarco_bert-base_german\"))\n",
    "german_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store_de, top_k=50))\n",
    "german_pipeline.add_component(\"ranker\", TransformersSimilarityRanker())\n",
    "german_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "german_pipeline.connect(\"retriever.documents\", \"ranker.documents\")\n",
    "\n",
    "\n",
    "template_en = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context: \n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}?\n",
    "\"\"\"\n",
    "\n",
    "template_de = \"\"\"\n",
    "Gegeben die folgenden Informationen, beantworte die Frage.\n",
    "\n",
    "Kontext:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Frage: {{ query }}?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "english_pipeline.add_component(\"prompt_builder\", PromptBuilder(template=template_en))\n",
    "english_pipeline.add_component(\"llm\", OpenAIGenerator(api_key=api_key))\n",
    "english_pipeline.connect(\"ranker\", \"prompt_builder.documents\")\n",
    "english_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "german_pipeline.add_component(\"prompt_builder\", PromptBuilder(template=template_de))\n",
    "german_pipeline.add_component(\"llm\", OpenAIGenerator(api_key=api_key))\n",
    "german_pipeline.connect(\"ranker\", \"prompt_builder.documents\")\n",
    "german_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "\n",
    "\n",
    "def ask_rag(query: str):\n",
    "  language = detect(query)\n",
    "  pipeline = german_pipeline if language == \"de\" else english_pipeline\n",
    "  \n",
    "  result = pipeline.run(\n",
    "      {\n",
    "        \"text_embedder\": {\"text\": query}, \n",
    "        \"ranker\": {\"query\": query, \"top_k\": 3},\n",
    "        \"prompt_builder\": {\"query\": query},\n",
    "        })\n",
    "  \n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.02it/s]\n"
     ]
    }
   ],
   "source": [
    "result = ask_rag(\"who is Tum's president\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llm': {'replies': [\"The name of TUM's president is Prof. Thomas F. Hofmann.\"], 'meta': [{'model': 'gpt-3.5-turbo-0613', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 16, 'prompt_tokens': 2382, 'total_tokens': 2398}}]}}\n"
     ]
    }
   ],
   "source": [
    "# print(result[\"llm\"][\"replies\"][0])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack-rag-showcase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
